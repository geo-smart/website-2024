{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system packages\n",
    "from datetime import datetime, date, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore import UNSIGNED\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Patch\n",
    "import math\n",
    "from evaluation_table import EvalTable\n",
    "\n",
    "# model packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "\n",
    "# Identify the path\n",
    "home = os.getcwd()\n",
    "parent_path = os.path.dirname(home)\n",
    "input_path = f'{parent_path}/02.input/'\n",
    "output_path = f'{parent_path}/03.output/'\n",
    "main_path = home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset and Model\n",
    "\n",
    "# List of station IDs that are of interest.\n",
    "stations = ['10126000', '10130500', '10134500', '10136500', '10137500', '10141000', '10155000', '10164500', '10171000']\n",
    "\n",
    "# Read a CSV file into a DataFrame and set the first column as the index.\n",
    "df = pd.read_parquet(f'{input_path}final_input.parquet')\n",
    "\n",
    "# Convert the station_id column to string data type.\n",
    "df.station_id = df.station_id.astype(str)\n",
    "\n",
    "# Convert the 'datetime' column to datetime objects.\n",
    "df.datetime = pd.to_datetime(df.datetime)\n",
    "\n",
    "# Filter the DataFrame to include only the rows where 'station_id' is in the 'stations' list.\n",
    "df_modified = df[df['station_id'].isin(stations)]\n",
    "with open(f\"{output_path}x_tes.pkl\", 'rb') as file:\n",
    "    x_test_scaled = pickle.load(file)\n",
    "\n",
    "with open(f\"{output_path}y_test.pkl\", 'rb') as file:\n",
    "    y_test_scaled = pickle.load(file)\n",
    "\n",
    "with open(f\"{output_path}best_model_xgboost.pkl\", 'rb') as file:\n",
    "    optimized_xgboost_model = pickle.load(file)\n",
    "\n",
    "scaler_y= joblib.load(f'{output_path}scaler_y.joblib')\n",
    "\n",
    "data_test = pd.read_pickle(f\"{output_path}test_dataset.pkl\")\n",
    "\n",
    "station_list = list(x_test_scaled.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5. Testing the Model\n",
    "We will give the model the test set for each station and compare it with the observation to evaluate the model with a dataset it has not seen before. Before feeding the test data we load the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize empty DataFrames to store evaluation results if not already defined.\n",
    "EvalDF_all_rf = pd.DataFrame()\n",
    "SupplyEvalDF_all_rf = pd.DataFrame()\n",
    "df_eval_rf = pd.DataFrame()\n",
    "\n",
    "# Iterate over each station name in the list of station IDs.\n",
    "for station_name in station_list:\n",
    "    # Retrieve scaled test features for the current station.\n",
    "    x_test_scaled_temp = x_test_scaled[station_name]\n",
    "    \n",
    "    # Make predictions using the scaled test features.\n",
    "    yhat_test_scaled = optimized_xgboost_model.predict(x_test_scaled_temp)\n",
    "    \n",
    "    # Inverse transform the scaled predictions to their original scale.\n",
    "    yhat_test = scaler_y.inverse_transform(yhat_test_scaled.reshape(-1, 1))\n",
    "    \n",
    "    # Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\n",
    "    EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp = EvalTable(yhat_test.reshape(-1), data_test[data_test.station_id == station_name], 'xgboost')\n",
    "\n",
    "    # Append the results from each station to the respective DataFrame.\n",
    "    EvalDF_all_rf = pd.concat([EvalDF_all_rf, EvalDF_all_rf_temp], ignore_index=True)\n",
    "    SupplyEvalDF_all_rf = pd.concat([SupplyEvalDF_all_rf, SupplyEvalDF_all_rf_temp], ignore_index=True)\n",
    "    df_eval_rf = pd.concat([df_eval_rf, df_eval_rf_temp], ignore_index=True)\n",
    "\n",
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all_rf)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalDF_all_rf.rename(columns={'USGSid': 'station_id'}, inplace=True)\n",
    "df_modified = df_modified[['station_id', 'Lat', 'Long']]\n",
    "df_modified = df_modified[['station_id', 'Lat', 'Long']].drop_duplicates().reset_index(drop=True)\n",
    "EvalDF_all_rf_all = pd.merge(EvalDF_all_rf, df_modified[['station_id', 'Lat', 'Long']], on='station_id')\n",
    "\n",
    "SupplyEvalDF_all_rf.rename(columns={'USGSid': 'station_id'}, inplace=True)\n",
    "SupplyEvalDF_all_rf_all = pd.merge(SupplyEvalDF_all_rf, df_modified[['station_id', 'Lat', 'Long']], on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_kge(kge):\n",
    "    if kge < 0:\n",
    "        return 0\n",
    "    elif 0 < kge <= 0.5:\n",
    "        return 1\n",
    "    elif 0.5 < kge <= 0.75:\n",
    "        return 2\n",
    "    elif 0.75 < kge :\n",
    "        return 3\n",
    "\n",
    "EvalDF_all_rf_all['NWM_KGE_cat'] = SupplyEvalDF_all_rf['NWM_KGE'].apply(categorize_kge)\n",
    "EvalDF_all_rf_all['xgboost__KGE_cat'] = SupplyEvalDF_all_rf['xgboost__KGE'].apply(categorize_kge)\n",
    "\n",
    "def categorize_pbias(pbias):\n",
    "    if -15 < pbias < 0:\n",
    "        return 0\n",
    "    elif pbias < -15:\n",
    "        return 1\n",
    "    elif 0 < pbias < 15:\n",
    "        return 2\n",
    "    elif 15 < pbias :\n",
    "        return 3    \n",
    "EvalDF_all_rf_all['NWM_PBias_cat'] = SupplyEvalDF_all_rf['NWM_PBias'].apply(categorize_pbias)\n",
    "EvalDF_all_rf_all['xgboost_PBias_cat'] = SupplyEvalDF_all_rf['xgboost_PBias'].apply(categorize_pbias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape_input = f'{input_path}shape/'\n",
    "\n",
    "file_list = ['jordan', 'weber', 'bear']\n",
    "# Load the shapefile\n",
    "\n",
    "for file_name in file_list:\n",
    "    gdf = gpd.read_file(f\"{shape_input}{file_name}.shp\")\n",
    "\n",
    "    # Merge all polygons into one\n",
    "    merged_polygon = gdf.unary_union\n",
    "\n",
    "    # Create a new GeoDataFrame\n",
    "    merged_gdf = gpd.GeoDataFrame(geometry=[merged_polygon], crs=gdf.crs)\n",
    "\n",
    "    # Save the merged polygon to a new shapefile\n",
    "    merged_gdf.to_file(f\"{shape_input}{file_name}_merged.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_gdf_bear = gpd.read_file(f\"{shape_input}river_bear.shp\")\n",
    "\n",
    "river_gdf_jordan_weber = gpd.read_file(f\"{shape_input}river_jordan_weber.shp\")\n",
    "\n",
    "lake_gdf_jordan_weber = gpd.read_file(f\"{shape_input}lake_jordan_weber.shp\")\n",
    "\n",
    "lake_gdf_bear = gpd.read_file(f\"{shape_input}lake_bear.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "year = ['NWM KGE', 'XGBoost KGE', 'NWM PBias', 'XGBoost PBias']\n",
    "name = 'Severity'\n",
    "variable = ['NWM_KGE_cat', 'xgboost__KGE_cat', 'NWM_PBias_cat', 'xgboost_PBias_cat']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 17), dpi=300, sharey=True, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_index, ax in enumerate(axes):\n",
    "    colors = ['fuchsia', 'black', 'green']\n",
    "\n",
    "    if ax_index < 2:\n",
    "        df_points = EvalDF_all_rf_all[['Lat', 'Long', 'NWM_KGE_cat', 'xgboost__KGE_cat']]\n",
    "    elif ax_index >= 2:\n",
    "        df_points = EvalDF_all_rf_all[['Lat', 'Long', 'NWM_PBias_cat', 'xgboost_PBias_cat']]\n",
    "\n",
    "    for file_name, color_name in zip(file_list, colors):\n",
    "        if color_name == 'fuchsia':\n",
    "            my_zorder = 5\n",
    "        else:\n",
    "            my_zorder = 1\n",
    "\n",
    "        merged_gdf = gpd.read_file(f\"{shape_input}{file_name}_merged.shp\", zorder=my_zorder)\n",
    "\n",
    "        merged_gdf.plot(ax=ax, alpha=0.9, facecolor='none', edgecolor=color_name, linewidth=1, label=file_name) \n",
    "    lats = df_points['Lat']  # Example latitudes\n",
    "    lons = df_points['Long']   # Example longitudes\n",
    "    values = df_points[f'{variable[ax_index]}']  # Values associated with each point\n",
    "\n",
    "    # Create GeoDataFrame from coordinates\n",
    "    points_data = gpd.GeoDataFrame({'Latitude': lats, 'Longitude': lons, 'Value': values},\n",
    "                                geometry=[Point(xy) for xy in zip(lons, lats)],\n",
    "                                crs=\"EPSG:4326\")  # Define the coordinate reference system\n",
    "\n",
    "    subset = river_gdf_bear[river_gdf_bear['StreamLeve'] == 4]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth= 1.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "    subset = river_gdf_bear[~(river_gdf_bear['StreamLeve'] == 4)]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth= 0.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "\n",
    "\n",
    "    subset = river_gdf_jordan_weber[(river_gdf_jordan_weber['StreamLeve'] == 4) & ((river_gdf_jordan_weber['StreamOrde'] == 5) | (river_gdf_jordan_weber['StreamOrde'] == 6))]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth= 1.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "    subset = river_gdf_jordan_weber[~((river_gdf_jordan_weber['StreamLeve'] == 4) & ((river_gdf_jordan_weber['StreamOrde'] == 5) | (river_gdf_jordan_weber['StreamOrde'] == 6)))]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth= 0.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "\n",
    "    # Define colors for each value\n",
    "    # value_colors = {0: 'yellow', 1: 'orange', 2: 'red'}\n",
    "\n",
    "    value_colors = {0: 'red', 1: 'orange', 2: 'lightgreen', 3: 'darkgreen'}\n",
    "\n",
    "\n",
    "    # Plot the points GeoDataFrame with colors based on the 'Value'\n",
    "    points_data.plot(ax=ax, marker='o', color=[value_colors[val] for val in points_data['Value']], markersize=50, label='Points', edgecolor='black', zorder=2)\n",
    "\n",
    "\n",
    "\n",
    "    subset_lake_gdf_jordan_weber = lake_gdf_jordan_weber[(lake_gdf_jordan_weber['GNIS_Name'] != None) & (lake_gdf_jordan_weber['AreaSqKm'] >= 5) & ((lake_gdf_jordan_weber['FType'] == 390) | (lake_gdf_jordan_weber['FType'] == 436))]\n",
    "    subset_lake_gdf_jordan_weber.plot(ax=ax, color='darkblue', linewidth= 0.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "\n",
    "\n",
    "\n",
    "    subset_lake_gdf_bear = lake_gdf_bear[(lake_gdf_bear['GNIS_Name'] != None) & (lake_gdf_bear['AreaSqKm'] >= 5) & ((lake_gdf_bear['FType'] == 390) | (lake_gdf_bear['FType'] == 436))]\n",
    "    subset_lake_gdf_bear.plot(ax=ax, color='darkblue', linewidth= 0.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "\n",
    "\n",
    "\n",
    "    # colur_name = ['Low', 'Medium', 'High']\n",
    "    if ax_index < 2:\n",
    "        colur_name = ['< 0', '0 - 0.5', '0.5 - 0.75', '0.75 <']\n",
    "    if ax_index >= 2:\n",
    "        colur_name = ['< -15', '-15 - 0', '0 - 15', '15 <']\n",
    "\n",
    "    # Create legend handles for sh\n",
    "    # legend_handles = [Patch(facecolor='none', edgecolor=color, label=label) for color, label in zip(colors, file_list)]\n",
    "    shapefile_handles = [Patch(facecolor='none', edgecolor=color, label=label) for color, label in zip(colors, file_list)]\n",
    "    point_handles = [Patch(facecolor=color, edgecolor='none', label=f'{colur_name[val]}') for val, color in value_colors.items()]\n",
    "    all_handles = shapefile_handles + point_handles\n",
    "    # Add titles and labels if necessary\n",
    "    ax.set_title(f'{year[ax_index]} ')\n",
    "    if ax_index == 0 or ax_index == 2:\n",
    "        ax.set_ylabel('Latitude')\n",
    "    if ax_index >= 2:\n",
    "        ax.set_xlabel('Longitude')\n",
    "\n",
    "    # Show the plot\n",
    "    ax.legend(handles=all_handles, loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'{output_path_general}map_{name}_{year[0]}_{durtation_num}.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
