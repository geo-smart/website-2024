{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "## Machine Learning for Post-Processing NWM Data \n",
    "**Authors: Savalan Naser Neisary (PhD Student, CIROH & The University of Alabama)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system packages\n",
    "from datetime import datetime, date, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore import UNSIGNED\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Patch\n",
    "import math\n",
    "from evaluation_table import EvalTable\n",
    "\n",
    "# model packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "\n",
    "# Identify the path\n",
    "home = os.getcwd()\n",
    "parent_path = os.path.dirname(home)\n",
    "input_path = f'{parent_path}/02.input/'\n",
    "output_path = f'{parent_path}/03.output/'\n",
    "main_path = home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset and Model\n",
    "\n",
    "# List of station IDs that are of interest.\n",
    "stations = ['10126000', '10130500', '10134500', '10136500', '10137500', '10141000', '10155000', '10164500', '10171000']\n",
    "\n",
    "# Read a CSV file into a DataFrame and set the first column as the index.\n",
    "df = pd.read_parquet(f'{input_path}final_input.parquet')\n",
    "\n",
    "# Convert the station_id column to string data type.\n",
    "df.station_id = df.station_id.astype(str)\n",
    "\n",
    "# Convert the 'datetime' column to datetime objects.\n",
    "df.datetime = pd.to_datetime(df.datetime)\n",
    "\n",
    "# Filter the DataFrame to include only the rows where 'station_id' is in the 'stations' list.\n",
    "df_modified = df[df['station_id'].isin(stations)]\n",
    "with open(f\"{output_path}x_tes.pkl\", 'rb') as file:\n",
    "    x_test_scaled = pickle.load(file)\n",
    "\n",
    "with open(f\"{output_path}y_test.pkl\", 'rb') as file:\n",
    "    y_test_scaled = pickle.load(file)\n",
    "\n",
    "with open(f\"{output_path}best_model_xgboost.pkl\", 'rb') as file:\n",
    "    optimized_xgboost_model = pickle.load(file)\n",
    "\n",
    "scaler_y= joblib.load(f'{output_path}scaler_y.joblib')\n",
    "\n",
    "data_test = pd.read_pickle(f\"{output_path}test_dataset.pkl\")\n",
    "\n",
    "station_list = list(x_test_scaled.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Development Continued\n",
    "#### 5.5. Testing the Model\n",
    "We will give the model the test set for each station and compare it with the observation to evaluate the model with a dataset it has not seen before. Before feeding the test data we load the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames to store evaluation results if not already defined.\n",
    "EvalDF_all_rf = pd.DataFrame()\n",
    "SupplyEvalDF_all_rf = pd.DataFrame()\n",
    "df_eval_rf = pd.DataFrame()\n",
    "final_output = {}\n",
    "\n",
    "# Iterate over each station name in the list of station IDs.\n",
    "for station_name in station_list:\n",
    "    # Retrieve scaled test features for the current station.\n",
    "    x_test_scaled_temp = x_test_scaled[station_name]\n",
    "    \n",
    "    # Make predictions using the scaled test features.\n",
    "    yhat_test_scaled = optimized_xgboost_model.predict(x_test_scaled_temp)\n",
    "    \n",
    "    # Inverse transform the scaled predictions to their original scale.\n",
    "    yhat_test = scaler_y.inverse_transform(yhat_test_scaled.reshape(-1, 1))\n",
    "\n",
    "    # Save the final output for plotting\n",
    "    final_output[station_name] = pd.DataFrame(yhat_test, columns=['pred_cfs'])\n",
    "    \n",
    "    # Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\n",
    "    EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp = EvalTable(yhat_test.reshape(-1), data_test[data_test.station_id == station_name], 'xgboost')\n",
    "\n",
    "    # Append the results from each station to the respective DataFrame.\n",
    "    EvalDF_all_rf = pd.concat([EvalDF_all_rf, EvalDF_all_rf_temp], ignore_index=True)\n",
    "    SupplyEvalDF_all_rf = pd.concat([SupplyEvalDF_all_rf, SupplyEvalDF_all_rf_temp], ignore_index=True)\n",
    "    df_eval_rf = pd.concat([df_eval_rf, df_eval_rf_temp], ignore_index=True)\n",
    "\n",
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all_rf)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalDF_all_rf.rename(columns={'USGSid': 'station_id'}, inplace=True)\n",
    "df_modified = df_modified[['station_id', 'Lat', 'Long']]\n",
    "df_modified = df_modified[['station_id', 'Lat', 'Long']].drop_duplicates().reset_index(drop=True)\n",
    "EvalDF_all_rf_all = pd.merge(EvalDF_all_rf, df_modified[['station_id', 'Lat', 'Long']], on='station_id')\n",
    "\n",
    "SupplyEvalDF_all_rf.rename(columns={'USGSid': 'station_id'}, inplace=True)\n",
    "SupplyEvalDF_all_rf_all = pd.merge(SupplyEvalDF_all_rf, df_modified[['station_id', 'Lat', 'Long']], on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The two stations with the highest and lowest improvement. \n",
    "\n",
    "two_list = ['10126000', '10136500']\n",
    "\n",
    "two_list_name = ['Low', 'High']\n",
    "\n",
    "# Initialize variables for the number of plots, columns, and rows based on the number of unique stations.\n",
    "n_subplots = 2\n",
    "n_cols = int(math.ceil(math.sqrt(n_subplots)))  # Calculate columns as the ceiling of the square root of number of subplots.\n",
    "n_rows = int(math.ceil(n_subplots / n_cols))  # Calculate rows as the ceiling of the ratio of subplots to columns.\n",
    "figsize = (15, 7)  # Set the figure size for the plot.\n",
    "# Create a figure and a grid of subplots with the specified number of rows and columns.\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "axes = axes.flatten()  # Flatten the axes array for easier iteration.\n",
    "\n",
    "# Iterate over each axis to plot data for each station.\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < n_subplots:  # Check if the current index is less than the number of subplots to populate.\n",
    "\n",
    "        # Get the observation data\n",
    "        obs = data_test[data_test.station_id == two_list[i]][['datetime', 'flow_cfs', 'NWM_flow']].reset_index(drop=True)\n",
    "\n",
    "        # Get the prediction data\n",
    "        pred = final_output[two_list[i]].reset_index(drop=True)\n",
    "\n",
    "        # Concat the two datastes\n",
    "        eval_data = pd.concat([obs, pred], axis=1)\n",
    "\n",
    "        # Set 'datetime' as the index for the DataFrame for plotting.\n",
    "        temp_df_2 = eval_data.set_index('datetime')\n",
    "        # Plot 'flow_cfs' on the primary y-axis.\n",
    "        ax.plot(temp_df_2.index, temp_df_2['flow_cfs'], label='Observation')\n",
    "        ax.plot(temp_df_2.index, temp_df_2['pred_cfs'], label='XGBoost Prediction')\n",
    "        ax.plot(temp_df_2.index, temp_df_2['NWM_flow'], label='NWM Prediction')\n",
    "        # Set x-axis limits from the minimum to maximum year of data.\n",
    "        start_year = pd.to_datetime(f'{eval_data.datetime.dt.year.min()}-01-01')\n",
    "        end_year = pd.to_datetime(f'{eval_data.datetime.dt.year.max()}-12-31')\n",
    "        ax.set_xlim(start_year, end_year)\n",
    "        # Get current x-tick labels and set their rotation for better visibility.\n",
    "        labels = ax.get_xticklabels()\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "\n",
    "        # Set the title of the subplot to the station ID.\n",
    "        ax.set_title(f'{two_list[i]} | {two_list_name[i]} Improvement')\n",
    "        # Set the x-axis label for subplots in the last row.\n",
    "        if i // n_cols == n_rows - 1:\n",
    "            ax.set_xlabel('Datetime (day)')\n",
    "\n",
    "        # Set the y-axis label for subplots in the first column.\n",
    "        if i % n_cols == 0:\n",
    "            ax.set_ylabel('Streamflow (cfs)')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        # Hide any unused axes.\n",
    "        ax.axis('off')\n",
    "        \n",
    "\n",
    "# Adjust the layout to prevent overlapping elements.\n",
    "plt.tight_layout()\n",
    "# Uncomment the line below to save the figure to a file.\n",
    "# plt.savefig(f'{save_path}scatter_annual_drought_number.png')\n",
    "# Display the plot.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_kge(kge):\n",
    "    if kge < 0:\n",
    "        return 0\n",
    "    elif 0 < kge <= 0.5:\n",
    "        return 1\n",
    "    elif 0.5 < kge <= 0.75:\n",
    "        return 2\n",
    "    elif 0.75 < kge :\n",
    "        return 3\n",
    "\n",
    "EvalDF_all_rf_all['NWM_KGE_cat'] = SupplyEvalDF_all_rf['NWM_KGE'].apply(categorize_kge)\n",
    "EvalDF_all_rf_all['xgboost__KGE_cat'] = SupplyEvalDF_all_rf['xgboost__KGE'].apply(categorize_kge)\n",
    "\n",
    "def categorize_pbias(pbias):\n",
    "    if -15 < pbias < 0:\n",
    "        return 0\n",
    "    elif pbias < -15:\n",
    "        return 1\n",
    "    elif 0 < pbias < 15:\n",
    "        return 2\n",
    "    elif 15 < pbias :\n",
    "        return 3    \n",
    "EvalDF_all_rf_all['NWM_PBias_cat'] = SupplyEvalDF_all_rf['NWM_PBias'].apply(categorize_pbias)\n",
    "EvalDF_all_rf_all['xgboost_PBias_cat'] = SupplyEvalDF_all_rf['xgboost_PBias'].apply(categorize_pbias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Specify the input path for your GeoPackage\n",
    "geopackage_input = f'{input_path}all_shapes.gpkg'\n",
    "\n",
    "file_list = ['jordan', 'weber', 'bear']\n",
    "\n",
    "# Load and process the shapefiles from the GeoPackage\n",
    "for file_name in file_list:\n",
    "    gdf = gpd.read_file(geopackage_input, layer=file_name)\n",
    "\n",
    "    # Merge all polygons into one\n",
    "    merged_polygon = gdf.unary_union\n",
    "\n",
    "    # Create a new GeoDataFrame\n",
    "    merged_gdf = gpd.GeoDataFrame(geometry=[merged_polygon], crs=gdf.crs)\n",
    "\n",
    "    # Save the merged polygon to a new layer in the GeoPackage\n",
    "    merged_gdf.to_file(geopackage_input, layer=f\"{file_name}_merged\", driver=\"GPKG\")\n",
    "\n",
    "# Load the other layers from the GeoPackage\n",
    "river_gdf_bear = gpd.read_file(geopackage_input, layer=\"river_bear\")\n",
    "river_gdf_jordan_weber = gpd.read_file(geopackage_input, layer=\"river_jordan_weber\")\n",
    "lake_gdf_jordan_weber = gpd.read_file(geopackage_input, layer=\"lake_jordan_weber\")\n",
    "lake_gdf_bear = gpd.read_file(geopackage_input, layer=\"lake_bear\")\n",
    "\n",
    "# Continue with the rest of your code unchanged...\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "year = ['NWM KGE', 'XGBoost KGE', 'NWM PBias', 'XGBoost PBias']\n",
    "name = 'Severity'\n",
    "variable = ['NWM_KGE_cat', 'xgboost__KGE_cat', 'NWM_PBias_cat', 'xgboost_PBias_cat']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 17), dpi=300, sharey=True, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_index, ax in enumerate(axes):\n",
    "    colors = ['fuchsia', 'black', 'green']\n",
    "\n",
    "    if ax_index < 2:\n",
    "        df_points = EvalDF_all_rf_all[['Lat', 'Long', 'NWM_KGE_cat', 'xgboost__KGE_cat']]\n",
    "    elif ax_index >= 2:\n",
    "        df_points = EvalDF_all_rf_all[['Lat', 'Long', 'NWM_PBias_cat', 'xgboost_PBias_cat']]\n",
    "\n",
    "    for file_name, color_name in zip(file_list, colors):\n",
    "        if color_name == 'fuchsia':\n",
    "            my_zorder = 5\n",
    "        else:\n",
    "            my_zorder = 1\n",
    "\n",
    "        merged_gdf = gpd.read_file(geopackage_input, layer=f\"{file_name}_merged\")\n",
    "\n",
    "        merged_gdf.plot(ax=ax, alpha=0.9, facecolor='none', edgecolor=color_name, linewidth=1, label=file_name)\n",
    "    \n",
    "    lats = df_points['Lat']  # Example latitudes\n",
    "    lons = df_points['Long']   # Example longitudes\n",
    "    values = df_points[f'{variable[ax_index]}']  # Values associated with each point\n",
    "\n",
    "    # Create GeoDataFrame from coordinates\n",
    "    points_data = gpd.GeoDataFrame({'Latitude': lats, 'Longitude': lons, 'Value': values},\n",
    "                                geometry=[Point(xy) for xy in zip(lons, lats)],\n",
    "                                crs=\"EPSG:4326\")  # Define the coordinate reference system\n",
    "\n",
    "    subset = river_gdf_bear[river_gdf_bear['StreamLeve'] == 4]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth=1.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "    subset = river_gdf_bear[~(river_gdf_bear['StreamLeve'] == 4)]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth=0.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "\n",
    "    subset = river_gdf_jordan_weber[(river_gdf_jordan_weber['StreamLeve'] == 4) & ((river_gdf_jordan_weber['StreamOrde'] == 5) | (river_gdf_jordan_weber['StreamOrde'] == 6))]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth=1.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "    subset = river_gdf_jordan_weber[~((river_gdf_jordan_weber['StreamLeve'] == 4) & ((river_gdf_jordan_weber['StreamOrde'] == 5) | (river_gdf_jordan_weber['StreamOrde'] == 6)))]\n",
    "    subset.plot(ax=ax, color='darkblue', linewidth=0.5, label=f'Stream Order', zorder=1)  # Multiply order by 2 for line width\n",
    "\n",
    "    # Define colors for each value\n",
    "    value_colors = {0: 'red', 1: 'orange', 2: 'lightgreen', 3: 'darkgreen'}\n",
    "\n",
    "    # Plot the points GeoDataFrame with colors based on the 'Value'\n",
    "    points_data.plot(ax=ax, marker='o', color=[value_colors[val] for val in points_data['Value']], markersize=50, label='Points', edgecolor='black', zorder=2)\n",
    "\n",
    "    subset_lake_gdf_jordan_weber = lake_gdf_jordan_weber[(lake_gdf_jordan_weber['GNIS_Name'] != None) & (lake_gdf_jordan_weber['AreaSqKm'] >= 5) & ((lake_gdf_jordan_weber['FType'] == 390) | (lake_gdf_jordan_weber['FType'] == 436))]\n",
    "    subset_lake_gdf_jordan_weber.plot(ax=ax, color='darkblue', linewidth=0.5, label=f'Stream Order', zorder=1)\n",
    "\n",
    "    subset_lake_gdf_bear = lake_gdf_bear[(lake_gdf_bear['GNIS_Name'] != None) & (lake_gdf_bear['AreaSqKm'] >= 5) & ((lake_gdf_bear['FType'] == 390) | (lake_gdf_bear['FType'] == 436))]\n",
    "    subset_lake_gdf_bear.plot(ax=ax, color='darkblue', linewidth=0.5, label=f'Stream Order', zorder=1)\n",
    "\n",
    "    if ax_index < 2:\n",
    "        colur_name = ['< 0', '0 - 0.5', '0.5 - 0.75', '0.75 <']\n",
    "    if ax_index >= 2:\n",
    "        colur_name = ['< -15', '-15 - 0', '0 - 15', '15 <']\n",
    "\n",
    "    shapefile_handles = [Patch(facecolor='none', edgecolor=color, label=label) for color, label in zip(colors, file_list)]\n",
    "    point_handles = [Patch(facecolor=color, edgecolor='none', label=f'{colur_name[val]}') for val, color in value_colors.items()]\n",
    "    all_handles = shapefile_handles + point_handles\n",
    "\n",
    "    ax.set_title(f'{year[ax_index]} ')\n",
    "    if ax_index == 0 or ax_index == 2:\n",
    "        ax.set_ylabel('Latitude')\n",
    "    if ax_index >= 2:\n",
    "        ax.set_xlabel('Longitude')\n",
    "\n",
    "    ax.legend(handles=all_handles, loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
